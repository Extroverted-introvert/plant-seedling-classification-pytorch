
# Plant Seedlings Classification: From Scratch to Transfer Learning

This project documents the end-to-end process of building, training, and optimizing a deep learning model to classify 12 species of plant seedlings. The journey progresses from a custom Convolutional Neural Network (CNN) built from scratch to a highly effective model using transfer learning and data augmentation with PyTorch.

The primary dataset used is the **V2 Plant Seedlings Dataset** from Aarhus University, which contains approximately 4,750 images across 12 different species. This dataset presents real-world challenges, including background noise (soil, weeds), high intra-class variation (plants at different growth stages), and inter-class similarity (different species looking alike).

## Project Journey & Methodology

The project is structured as a series of four Jupyter notebooks, each building upon the last to demonstrate a typical machine learning workflow.

1.  **[NB01] Data Exploration and Setup:** We begin by loading the dataset, performing an exploratory data analysis (EDA) to understand its characteristics, and visualizing the class distribution. A crucial step here is creating a standardized train/validation/test split and establishing a consistent preprocessing pipeline (resizing to 224x224 and normalizing with ImageNet stats) to prepare for transfer learning.

2.  **[NB02] CNN from Scratch:** A baseline model is established by building a custom LeNet-style CNN. This model, trained from scratch, helps us understand the inherent challenges of the dataset and provides a performance benchmark. While achieving moderate accuracy, it shows clear signs of overfitting, highlighting the limitations of training deep networks on small datasets.

3.  **[NB03] Transfer Learning with ResNet18:** To overcome the limitations of the scratch model, we employ transfer learning. We use a **ResNet18** model pretrained on ImageNet, freezing its convolutional backbone and training only a new, custom classifier head. This "feature extraction" approach dramatically improves accuracy and training efficiency, proving the power of leveraging pre-learned features.

4.  **[NB04] Data Augmentation for Robustness:** In the final notebook, we enhance the transfer learning model by introducing data augmentation. By applying random transformations like flips, rotations, and color jitter during training, we artificially expand the dataset. This forces the model to learn more robust and generalizable features, making it less sensitive to variations in orientation and lighting.

## Repository Structure

```
.
├── 01_data_exploration_and_setup.ipynb
├── 02_cnn_from_scratch.ipynb
├── 03_transfer_learning_resnet.ipynb
├── 04_data_augmentation.ipynb
├── data/
│   ├── plant-seedlings/
│   │   ├── data/          <-- Original dataset from Kaggle goes here
│   │   └── data_split/    <-- Generated by Notebook 01
│   │       ├── train/
│   │       ├── val/
│   │       └── test/
└── README.md
```

*   `01_data_exploration_and_setup.ipynb`: Loads data, performs EDA, and creates train/val/test splits.
*   `02_cnn_from_scratch.ipynb`: Defines, trains, and evaluates a baseline CNN from scratch.
*   `03_transfer_learning_resnet.ipynb`: Implements transfer learning with a pretrained ResNet18 as a feature extractor.
*   `04_data_augmentation.ipynb`: Retrains the ResNet18 model with data augmentation to improve generalization.
*   `data/`: Directory to store the dataset. The `data_split` sub-directory is automatically created by the first notebook.

## Key Concepts & Technologies

*   **Libraries:** PyTorch, TorchVision, Scikit-learn, NumPy, Matplotlib, Seaborn
*   **Deep Learning Techniques:**
    *   Custom Convolutional Neural Network (CNN) design
    *   Transfer Learning (Feature Extraction)
    *   Data Augmentation (spatial and color transformations)
    *   Image Preprocessing and Normalization
    *   Model Evaluation (Accuracy, Per-Class Metrics, Confusion Matrix, Classification Report)
    *   Handling Data Splits for Training, Validation, and Testing

## Performance Summary

The progressive approach demonstrates a significant improvement in performance at each stage.

| Model | Trainable Parameters | Test Accuracy | Key Finding |
| :--- | :--- | :--- | :--- |
| **Custom CNN** (from Scratch) | ~25.8 Million | ~68% | Prone to severe overfitting on a small dataset. |
| **ResNet18** (Transfer Learning) | ~6,000 | ~85% | Pretrained features provide a massive performance boost and reduce overfitting. |
| **ResNet18** (+ Data Augmentation) | ~6,000 | **~88%** | Augmentation improves model robustness and generalization, leading to the best performance. |

<img src="https://i.imgur.com/example.png" alt="Per-class accuracy chart or confusion matrix could be embedded here">

## How to Run This Project

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/plant-seedlings-classification.git
    cd plant-seedlings-classification
    ```

2.  **Download the dataset:**
    *   Download the data from the [Kaggle: Plant Seedlings Classification](https://www.kaggle.com/competitions/plant-seedlings-classification/data) competition.
    *   Unzip the contents into the `data/plant-seedlings/data/` directory. The structure should look like `data/plant-seedlings/data/Black-grass/`, `data/plant-seedlings/data/Charlock/`, etc.

3.  **Install dependencies:**
    *   It is recommended to create a virtual environment.
    ```bash
    pip install torch torchvision scikit-learn numpy matplotlib seaborn jupyter
    ```

4.  **Run the notebooks:**
    *   Start by running `01_data_exploration_and_setup.ipynb` to create the necessary data splits.
    *   Proceed to run notebooks `02` through `04` in order.

## Conclusion & Future Work

This project successfully demonstrates how to tackle a real-world image classification problem by progressing from a simple baseline to an optimized transfer learning model. The key takeaway is that for small-to-medium sized datasets, transfer learning combined with data augmentation is a highly effective strategy.

Potential areas for future exploration include:

*   **Fine-Tuning:** Unfreezing some of the later layers of the ResNet18 backbone and training them with a very low learning rate.
*   **Advanced Architectures:** Experimenting with more modern pretrained models like EfficientNet or Vision Transformers (ViT).
*   **Imbalance Handling:** Employing techniques like weighted loss functions or class-aware sampling to better handle the imbalanced class distribution.
*   **Model Deployment:** Optimizing the final model using techniques like quantization or pruning and deploying it as a web service or mobile application.